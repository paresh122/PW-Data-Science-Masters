{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Assignment  72: KNN 1 - Utkarsh Gaikwad</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Assignment pdf link](./20%20Apr_AssQ.pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 1</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 : What is the KNN algorithm?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The K-Nearest Neighbors (KNN) algorithm is a type of machine learning algorithm that is used for classification and regression. It is a non-parametric and lazy learning algorithm, which means that it does not make any assumptions about the underlying data distribution and does not try to learn a model from the training data. Instead, it simply memorizes the training dataset and uses it to make predictions on new data.\n",
    "\n",
    "### The KNN algorithm works by finding the K nearest neighbors to a given data point in the feature space. The distance between the data points is typically measured using Euclidean distance, although other distance metrics can also be used. Once the K nearest neighbors have been identified, the algorithm assigns the class label of the majority of these neighbors to the data point being classified. In the case of regression, the algorithm predicts the average value of the K nearest neighbors.\n",
    "\n",
    "### The value of K is a hyperparameter that can be set by the user. A larger value of K will result in a smoother decision boundary, but may also lead to misclassification of data points that are close to the boundary. On the other hand, a smaller value of K will result in a more complex decision boundary, but may also lead to overfitting and poor generalization to new data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![KNN](https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs42452-019-1329-z/MediaObjects/42452_2019_1329_Fig1_HTML.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 2</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 : How do you choose the value of K in KNN?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the value of K in K-Nearest Neighbors (KNN) algorithm is an important step in achieving optimal results. The value of K determines the number of nearest neighbors to consider when making a prediction for a new data point. Here are some common ways to choose the value of K:\n",
    "\n",
    "1. `Cross-validation`: One of the most common approaches to select the value of K is through cross-validation. In this approach, you divide your data into training and validation sets. Then, for each value of K, you train your model on the training set and evaluate its performance on the validation set. You choose the value of K that gives the best performance on the validation set.\n",
    "\n",
    "2. `Rule of thumb`: Another simple approach is to use a rule of thumb that suggests the value of K as the square root of the number of data points. This is not always the optimal value, but it can be a good starting point.\n",
    "\n",
    "3. `Domain knowledge`: Sometimes, domain knowledge can help in selecting the value of K. For example, if you know that the dataset has a specific pattern or structure, you can choose the value of K accordingly.\n",
    "\n",
    "4. `Experimentation`: Finally, you can experiment with different values of K and compare their performance on your data. This approach is useful when you don't have prior knowledge of the optimal value of K.\n",
    "\n",
    "### In practice, it's important to try different values of K and evaluate their performance on your specific dataset to choose the optimal value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 3</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 : What is the difference between KNN classifier and KNN regressor?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN) algorithm can be used for both classification and regression tasks. Here's a tabular comparison between KNN classifier and KNN regressor:\n",
    "\n",
    "| KNN Classifier | KNN Regressor |\n",
    "| --- | --- |\n",
    "| KNN Classifier is used for classification tasks, where the goal is to predict a class label for a new data point. | KNN Regressor is used for regression tasks, where the goal is to predict a continuous value for a new data point. |\n",
    "| The output of KNN Classifier is a class label. | The output of KNN Regressor is a continuous value. |\n",
    "| The distance metric used in KNN Classifier is often Euclidean distance, but other distance metrics can also be used. | The distance metric used in KNN Regressor is often Euclidean distance, but other distance metrics can also be used. |\n",
    "| The predicted class label is determined by the majority class of the K nearest neighbors. | The predicted value is determined by the mean or median value of the K nearest neighbors. |\n",
    "| KNN Classifier is sensitive to the choice of K value, the distance metric used, and the choice of features. | KNN Regressor is also sensitive to the choice of K value, the distance metric used, and the choice of features. |\n",
    "| KNN Classifier can handle multi-class classification tasks. | KNN Regressor can handle both uni-variate and multi-variate regression tasks. |\n",
    "\n",
    "### In summary, KNN classifier is used for classification tasks, and KNN regressor is used for regression tasks. The output of KNN classifier is a class label, while the output of KNN regressor is a continuous value. The choice of K value, distance metric used, and feature selection are important in both cases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 4</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 : How do you measure the performance of KNN?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To measure the performance of K-Nearest Neighbors (KNN), you need to evaluate how well the model is able to make accurate predictions on new, unseen data. Here are some common performance metrics for KNN:\n",
    "\n",
    "1. Classification Metrics:\n",
    "    * Accuracy: the proportion of correctly classified instances out of the total number of instances.\n",
    "    * Precision: the proportion of true positive predictions out of the total number of positive predictions.\n",
    "    * Recall: the proportion of true positive predictions out of the total number of actual positive instances in the data.\n",
    "    * F1 score: the harmonic mean of precision and recall.\n",
    "    * Area Under the Receiver Operating Characteristic curve (AUC-ROC): a measure of how well the model is able to distinguish between positive and negative instances.    \n",
    "\n",
    "2. Regression Metrics:\n",
    "    * Mean Absolute Error (MAE): the average absolute difference between the predicted and actual values.\n",
    "    * Mean Squared Error (MSE): the average squared difference between the predicted and actual values.\n",
    "    * Root Mean Squared Error (RMSE): the square root of the average squared difference between the predicted and actual values.\n",
    "\n",
    "### To compute these metrics, you typically split your data into training and testing sets, train your KNN model on the training set, and evaluate its performance on the testing set. You can also use techniques such as k-fold cross-validation or leave-one-out cross-validation to get a more accurate estimate of the model's performance.\n",
    "\n",
    "### In practice, the choice of performance metric depends on the specific problem you are trying to solve and the objectives of your model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 5</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 : What is the curse of dimensionality in KNN?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The curse of dimensionality in K-Nearest Neighbors (KNN) refers to the phenomenon where the performance of the KNN algorithm deteriorates as the number of features or dimensions increases. Specifically, as the number of features or dimensions increases, the data becomes more sparse in the high-dimensional space, making it difficult to identify the nearest neighbors accurately. This results in a higher risk of misclassification or regression errors.\n",
    "\n",
    "### In high-dimensional space, the volume of the space increases exponentially with the number of dimensions, which means that the number of training instances needed to maintain a certain level of representation increases exponentially as well. As a result, KNN tends to become computationally expensive and memory-intensive as the number of dimensions increases.\n",
    "\n",
    "### To overcome the curse of dimensionality in KNN, various techniques have been proposed, such as feature selection or dimensionality reduction methods, which aim to reduce the number of irrelevant or redundant features in the data. Another approach is to use distance metrics that are more suitable for high-dimensional data, such as cosine similarity or Mahalanobis distance. Alternatively, other machine learning algorithms, such as decision trees or neural networks, may be more appropriate for high-dimensional data with complex relationships between features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 6</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 : How do you handle missing values in KNN?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN) is a distance-based algorithm that requires complete data with no missing values. Therefore, handling missing values is an important step in using KNN effectively. Here are some common approaches for handling missing values in KNN:\n",
    "\n",
    "1. `Deletion`: This approach involves deleting any data points with missing values from the dataset. This method is simple, but it can lead to a loss of valuable information and reduce the size of the dataset.\n",
    "\n",
    "2. `Imputation`: Imputation involves replacing missing values with estimated values based on the available data. There are several techniques for imputing missing values, including mean imputation, median imputation, and mode imputation. For example, you can replace missing values with the mean value of the feature across all other data points.\n",
    "\n",
    "3. `KNN-based imputation`: This approach involves using KNN to estimate the missing values based on the values of the nearest neighbors. In this method, the distance metric used for finding the nearest neighbors should be carefully chosen to handle missing values appropriately. For example, you can use mean imputation to fill in missing values before calculating distances.\n",
    "\n",
    "4. `Model-based imputation`: This approach involves building a model to predict missing values based on the available data. For example, you can use regression or decision trees to predict missing values based on the values of other features.\n",
    "\n",
    "### The choice of approach depends on the specific problem and the amount and pattern of missing data. In general, KNN-based imputation and model-based imputation tend to be more accurate than simple imputation methods, but they may also be more computationally intensive and require more training data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 7</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 : Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN) is a flexible machine learning algorithm that can be used for both classification and regression tasks. However, the performance of KNN classifier and regressor can differ significantly depending on the nature of the problem and the characteristics of the data. Here are some key differences between KNN classifier and regressor:\n",
    "\n",
    "1. `Output`: KNN classifier outputs discrete class labels, whereas KNN regressor outputs continuous numeric values.\n",
    "\n",
    "2. `Performance metrics`: The performance metrics used to evaluate KNN classifier and regressor are different. For classification tasks, metrics such as accuracy, precision, recall, and F1 score are commonly used. For regression tasks, metrics such as mean absolute error (MAE), mean squared error (MSE), and root mean squared error (RMSE) are commonly used.\n",
    "\n",
    "3. `Handling outliers`: KNN regressor can be sensitive to outliers in the data, as the prediction is based on the average value of the k-nearest neighbors. On the other hand, KNN classifier is less affected by outliers as long as the majority of the neighbors are correctly classified.\n",
    "\n",
    "4. `Data distribution`: KNN classifier works well when the classes are well separated, while KNN regressor works well when the data points are distributed smoothly.\n",
    "\n",
    "### Based on these differences, KNN classifier is generally better suited for classification problems with discrete class labels and well-separated classes. Examples include image classification, sentiment analysis, and spam detection. On the other hand, KNN regressor is better suited for regression problems with continuous numeric values and smoothly distributed data. Examples include predicting housing prices, stock prices, and temperature forecasting.\n",
    "\n",
    "### However, the choice of KNN classifier or regressor ultimately depends on the specific problem and the characteristics of the data. It is recommended to experiment with both algorithms and compare their performance using appropriate evaluation metrics before making a final decision."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 8</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qusestion 8 : What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN) is a popular and simple machine learning algorithm that can be used for classification and regression tasks. However, like any algorithm, KNN has its strengths and weaknesses.\n",
    "\n",
    "| Strengths of KNN | Weaknesses of KNN |\n",
    "| --- | --- |\n",
    "| Non-parametric: KNN is flexible and makes no assumptions about the data distribution. | Sensitive to outliers: KNN can be affected by outliers in the data. |\n",
    "| Easy to implement: KNN is relatively simple to implement and requires no training time. | Curse of dimensionality: KNN can suffer from poor performance when the number of features is large. |\n",
    "| Can handle complex relationships: KNN can capture non-linear relationships between features. | Choice of k-value: Selecting the optimal k-value can be challenging and can have a significant impact on performance. |\n",
    "| Works well with small datasets: KNN is effective with small datasets, as computation time and memory requirements are relatively low. | Requires complete data: KNN requires complete data, and missing values must be handled before applying the algorithm. |\n",
    "\n",
    "### To address these weaknesses, several techniques can be used. For example, outliers can be removed from the dataset or treated as a separate class. To address the curse of dimensionality, feature selection or dimensionality reduction techniques can be applied. Cross-validation and grid search can be used to select the optimal k-value. Finally, missing data can be imputed using various imputation techniques before applying the algorithm.\n",
    "\n",
    "### Overall, KNN is a powerful algorithm that can perform well for classification and regression tasks, but it requires careful consideration and handling of its limitations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 9</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9 : What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean distance and Manhattan distance are two commonly used distance metrics in KNN algorithm for finding the k nearest neighbors. The main difference between them lies in how they measure distance between two points.\n",
    "\n",
    "### Euclidean distance measures the shortest straight-line distance between two points in Euclidean space. It is calculated as the square root of the sum of the squared differences between the coordinates of the two points:\n",
    "\n",
    "~~~\n",
    "Euclidean distance = sqrt((x2 - x1)^2 + (y2 - y1)^2 + ... + (nk - nk-1)^2)\n",
    "where (x1, y1, ..., nk-1) and (x2, y2, ..., nk) are the coordinates of the two points in n-dimensional space.\n",
    "~~~\n",
    "\n",
    "### On the other hand, Manhattan distance measures the distance between two points by summing the absolute differences between their coordinates. It is calculated as follows:\n",
    "\n",
    "~~~\n",
    "Manhattan distance = |x2 - x1| + |y2 - y1| + ... + |nk - nk-1|\n",
    "where (x1, y1, ..., nk-1) and (x2, y2, ..., nk) are the coordinates of the two points in n-dimensional space.\n",
    "~~~ \n",
    "\n",
    "### The key difference between these two distance metrics is that Euclidean distance measures the direct distance between two points, while Manhattan distance measures the distance along the edges of the n-dimensional space. As a result, Euclidean distance tends to work well when the data is densely distributed, while Manhattan distance tends to work well when the data is sparse and the dimensions are not strongly correlated.\n",
    "\n",
    "### In summary, Euclidean distance and Manhattan distance are two different ways of measuring distance between two points in n-dimensional space, and the choice between them depends on the characteristics of the dataset and the problem at hand."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 10</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10 : What is the role of feature scaling in KNN?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling is an important step in the KNN algorithm, as it can have a significant impact on the performance of the algorithm. The reason for this is that KNN algorithm calculates the distance between data points to identify the k nearest neighbors. If the features are not scaled properly, then features with larger ranges can dominate the distance calculation, leading to biased results. Therefore, it is essential to scale the features to ensure that each feature contributes equally to the distance calculation.\n",
    "\n",
    "### There are different methods for feature scaling, including standardization and normalization. Standardization involves transforming the data so that it has zero mean and unit variance. This can be done by subtracting the mean of the feature from each value and dividing by the standard deviation. Normalization involves scaling the features so that they have a range of [0,1] or [-1,1]. This can be done by subtracting the minimum value of the feature from each value and dividing by the range of the feature.\n",
    "\n",
    "### By scaling the features, we ensure that the features are on the same scale and have the same impact on the distance calculation. This can improve the accuracy of the KNN algorithm and help to identify the true nearest neighbors. Without proper feature scaling, the KNN algorithm may not perform well and may lead to incorrect predictions or classifications."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
